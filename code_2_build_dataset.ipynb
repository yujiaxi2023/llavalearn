{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/yujiaxi/DLModels/llavalearn/data/liuhaotian/LLaVA-CC3M-Pretrain-595K\"\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlavaDataset(Dataset):\n",
    "    def __init__(self, dataset_dir: str) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.chat_data, self.image_dir = self.build_dataset(dataset_dir)\n",
    "\n",
    "    def build_dataset(self, data_dir:str) -> tuple[List, Path]:\n",
    "        data_dir = Path(data_dir)\n",
    "        chat_file = data_dir.joinpath(\"chat.json\")\n",
    "        image_dir = data_dir.joinpath(\"images-dl\")\n",
    "\n",
    "        chat_data = pd.read_json(chat_file).to_dict(orient=\"records\")\n",
    "\n",
    "        return chat_data, image_dir\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.chat_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        cur_data = self.chat_data[index]\n",
    "\n",
    "        human_input = cur_data['conversations'][0]['value']\n",
    "        gpt_output = cur_data['conversations'][1]['value']\n",
    "\n",
    "        image_path = self.image_dir.joinpath(cur_data.get('image'))\n",
    "\n",
    "        return (human_input, gpt_output, image_path)\n",
    "    \n",
    "test_llavatest = LlavaDataset(dataset_dir=data_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_llavatest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_llavatest[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(test_llavatest[19][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "llava_model_name_or_path = \"/home/yujiaxi/DLModels/llavalearn/show_model/model001\"\n",
    "llava_processor = AutoProcessor.from_pretrained(llava_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test002 = test_llavatest[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class QaImageOutput:\n",
    "    q_input_ids: torch.Tensor\n",
    "    pixel_values:torch.Tensor\n",
    "    a_input_ids:torch.Tensor\n",
    "\n",
    "\n",
    "def build_qaimage(processor: AutoProcessor, q_text:str, a_text:str, image_path: Path):\n",
    "    \n",
    "    # instruction or input or question\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": q_text},\n",
    "    ]\n",
    "    prompt = processor.tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    image_file = image_path\n",
    "    raw_image = Image.open(image_file)\n",
    "\n",
    "    inputs = processor(prompt, raw_image, return_tensors=\"pt\")\n",
    "\n",
    "    a_input_ids = processor.tokenizer(\n",
    "        a_text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\",\n",
    "        truncation=True,\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    return QaImageOutput(\n",
    "        q_input_ids=inputs['input_ids'],\n",
    "        pixel_values=inputs[\"pixel_values\"],\n",
    "        a_input_ids=a_input_ids\n",
    "        )  \n",
    "    \n",
    "\n",
    "c = build_qaimage(llava_processor, test002[0], test002[1], test002[2])\n",
    "c #.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.q_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llava_processor.decode([90527, 12452,  1273,  9606,  1154,  1573,   279, 10778,   389,  6775,\n",
    "                        8039,   659])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class TrainLlavaModelCollector:\n",
    "    def __init__(self, processor:AutoProcessor, IGNORE_INDEX:int) -> None:\n",
    "        self.processor = processor\n",
    "        self.ignore_index = IGNORE_INDEX\n",
    "\n",
    "    def convert_one_piece(self,\n",
    "                          q_input_ids:torch.Tensor,\n",
    "                          a_input_ids:torch.Tensor):\n",
    "        input_ids = torch.concat([\n",
    "            q_input_ids,\n",
    "            a_input_ids,\n",
    "            torch.tensor(self.processor.tokenizer.eos_token_id).reshape(1, -1)\n",
    "        ], dim=1)\n",
    "        labels = torch.concat([\n",
    "            torch.full_like(q_input_ids, fill_value=self.ignore_index),\n",
    "            a_input_ids,\n",
    "            torch.tensor(self.processor.tokenizer.eos_token_id).reshape(1, -1)\n",
    "        ], dim=1)\n",
    "\n",
    "        return input_ids, labels\n",
    "    \n",
    "    def __call__(self, features:List) -> Any:\n",
    "        \n",
    "        input_ids_list = []\n",
    "        labels_list = []\n",
    "        pixel_values = []\n",
    "        max_input_len_list = []\n",
    "\n",
    "        for feature in features:\n",
    "            qaimage_output = build_qaimage(\n",
    "                self.processor,\n",
    "                feature[0],\n",
    "                feature[1],\n",
    "                feature[2]\n",
    "            )\n",
    "            temp_input_ids, temp_labels = self.convert_one_piece(\n",
    "                qaimage_output.q_input_ids,\n",
    "                qaimage_output.a_input_ids\n",
    "            )\n",
    "            max_input_len_list.append(temp_input_ids.shape[1])\n",
    "            input_ids_list.append(temp_input_ids)\n",
    "            labels_list.append(temp_labels)\n",
    "            pixel_values.append(qaimage_output.pixel_values)\n",
    "\n",
    "        max_input_len = max(max_input_len_list)\n",
    "\n",
    "        final_input_ids = torch.concat([\n",
    "            torch.concat([\n",
    "                torch.full(\n",
    "                    (1, max_input_len - max_input_len_list[index]),\n",
    "                    self.processor.tokenizer.pad_token_id,\n",
    "                ),value,\n",
    "            ], axis=1)\n",
    "            for index, value in enumerate(input_ids_list)\n",
    "        ])\n",
    "        \n",
    "        final_labels = torch.concat([\n",
    "            torch.concat([\n",
    "                torch.full(\n",
    "                    (1, max_input_len - max_input_len_list[index]),\n",
    "                    self.processor.tokenizer.pad_token_id,\n",
    "                ),value,\n",
    "            ], axis=1)\n",
    "            for index, value in enumerate(labels_list)\n",
    "        ])\n",
    "\n",
    "        final_pixel_values = torch.concat(pixel_values, axis=0)\n",
    "        \n",
    "        attention_mask = torch.ones_like(final_input_ids)\n",
    "        attention_mask[final_input_ids == self.processor.tokenizer.pad_token_id] = 0\n",
    "\n",
    "        return {\n",
    "            \"input_ids\":final_input_ids,\n",
    "            \"labels\":final_labels,\n",
    "            \"pixel_values\":final_pixel_values,\n",
    "            \"attention_mask\":attention_mask\n",
    "        }\n",
    "\n",
    "tlmc = TrainLlavaModelCollector(llava_processor, -100)\n",
    "# tlmc.convert_one_piece(c.q_input_ids, c.a_input_ids)\n",
    "\n",
    "d = tlmc([test_llavatest[13],])\n",
    "d.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d[\"input_ids\"].shape\n",
    "# d[\"input_ids\"]\n",
    "d[\"labels\"]\n",
    "d[\"input_ids\"]\n",
    "d[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "llava_model_name_or_path = \"show_model/model001\"\n",
    "llava_model = LlavaForConditionalGeneration.from_pretrained(llava_model_name_or_path,\n",
    "                                                            torch_dtype=torch.bfloat16,\n",
    "                                                            device_map='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tk in d.keys():\n",
    "    d[tk] = d[tk].to(llava_model.device)\n",
    "\n",
    "model_output = llava_model(**d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output.loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
